{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prcatical3(DL).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQ4w53ZmPp29+ZhUQilCuI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/komalmagar/Image-recognation-using-DL/blob/main/Prcatical3(DL).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onlFPz36RN4x"
      },
      "source": [
        "import numpy as np\r\n",
        "from keras.layers import Input, Dense\r\n",
        "from keras.models import Model\r\n",
        "from keras.datasets import mnist  #To import dataset\r\n",
        "import matplotlib.pyplot as plt  ## for ploting data on graph\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM25283HRdzT"
      },
      "source": [
        "encoding_dim = 32 # size of inpute data n float\r\n",
        "input_img = Input(shape=(784,)) ## size of input placeholder\r\n",
        "encoded = Dense(encoding_dim, activation='relu')(input_img)# representation for input\r\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)# decodation for output \r\n",
        "autoencoder = Model(input_img, decoded)## modeling the data\r\n",
        "encoder = Model(input_img, encoded)  ## map input to desired output\r\n",
        "encoded_input = Input(shape=(encoding_dim,))## place holder for input\r\n",
        "decoder_layer = autoencoder.layers[-1] ## get has row data by using [-1]\r\n",
        "decoder = Model(encoded_input, decoder_layer(encoded_input))# creating decoding model\r\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy') #Compiling model\r\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3sUmGwlRjow",
        "outputId": "77b7a765-6a0d-40aa-8c3c-8df96a419516"
      },
      "source": [
        "(x_train, _), (x_test, _) = mnist.load_data()  ##loading data\r\n",
        "x_train = x_train.astype('float32') / 255.# normalise all value between 0 and 1\r\n",
        "x_test = x_test.astype('float32') / 255.\r\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\r\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\r\n",
        "print (x_train.shape)\r\n",
        "print (x_test.shape)\r\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84UegdSaRj_n",
        "outputId": "843f7d0c-b19b-4046-f0ff-6f3c0c061e90"
      },
      "source": [
        "autoencoder.fit(x_train, x_train,epochs=60,batch_size=256,shuffle=True,validation_data=(x_test, x_test))# FITTING THE MODEL with 50 iterations to update the weights for test data\r\n",
        "cv = KFold(n_splits=10, random_state=1, shuffle=True)\r\n",
        "encoded_imgs = encoder.predict(x_test)# encoding image for test data \r\n",
        "decoded_imgs = decoder.predict(encoded_imgs)# decoding for test data"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.3848 - val_loss: 0.1867\n",
            "Epoch 2/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1775 - val_loss: 0.1522\n",
            "Epoch 3/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1488 - val_loss: 0.1344\n",
            "Epoch 4/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1324 - val_loss: 0.1219\n",
            "Epoch 5/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1210 - val_loss: 0.1136\n",
            "Epoch 6/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1132 - val_loss: 0.1077\n",
            "Epoch 7/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1080 - val_loss: 0.1034\n",
            "Epoch 8/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1039 - val_loss: 0.1002\n",
            "Epoch 9/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1008 - val_loss: 0.0977\n",
            "Epoch 10/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0984 - val_loss: 0.0959\n",
            "Epoch 11/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0968 - val_loss: 0.0948\n",
            "Epoch 12/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0958 - val_loss: 0.0940\n",
            "Epoch 13/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0951 - val_loss: 0.0936\n",
            "Epoch 14/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0946 - val_loss: 0.0932\n",
            "Epoch 15/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0944 - val_loss: 0.0929\n",
            "Epoch 16/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0939 - val_loss: 0.0927\n",
            "Epoch 17/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0940 - val_loss: 0.0925\n",
            "Epoch 18/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0937 - val_loss: 0.0924\n",
            "Epoch 19/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0936 - val_loss: 0.0923\n",
            "Epoch 20/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0936 - val_loss: 0.0922\n",
            "Epoch 21/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0933 - val_loss: 0.0921\n",
            "Epoch 22/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0935 - val_loss: 0.0921\n",
            "Epoch 23/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0933 - val_loss: 0.0920\n",
            "Epoch 24/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0932 - val_loss: 0.0919\n",
            "Epoch 25/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0933 - val_loss: 0.0919\n",
            "Epoch 26/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0929 - val_loss: 0.0919\n",
            "Epoch 27/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0930 - val_loss: 0.0919\n",
            "Epoch 28/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0929 - val_loss: 0.0918\n",
            "Epoch 29/60\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.0930 - val_loss: 0.0918\n",
            "Epoch 30/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0930 - val_loss: 0.0918\n",
            "Epoch 31/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0930 - val_loss: 0.0918\n",
            "Epoch 32/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0930 - val_loss: 0.0918\n",
            "Epoch 33/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0928 - val_loss: 0.0917\n",
            "Epoch 34/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0927 - val_loss: 0.0917\n",
            "Epoch 35/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0925 - val_loss: 0.0918\n",
            "Epoch 36/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0928 - val_loss: 0.0917\n",
            "Epoch 37/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0927 - val_loss: 0.0917\n",
            "Epoch 38/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 39/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 40/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 41/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0926 - val_loss: 0.0916\n",
            "Epoch 42/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0924 - val_loss: 0.0916\n",
            "Epoch 43/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 44/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0928 - val_loss: 0.0916\n",
            "Epoch 45/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 46/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0927 - val_loss: 0.0915\n",
            "Epoch 47/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0928 - val_loss: 0.0916\n",
            "Epoch 48/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0928 - val_loss: 0.0915\n",
            "Epoch 49/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0924 - val_loss: 0.0915\n",
            "Epoch 50/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0928 - val_loss: 0.0915\n",
            "Epoch 51/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0927 - val_loss: 0.0915\n",
            "Epoch 52/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 53/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0924 - val_loss: 0.0915\n",
            "Epoch 54/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 55/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0926 - val_loss: 0.0914\n",
            "Epoch 56/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0925 - val_loss: 0.0916\n",
            "Epoch 57/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0925 - val_loss: 0.0914\n",
            "Epoch 58/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0925 - val_loss: 0.0915\n",
            "Epoch 59/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 60/60\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0927 - val_loss: 0.0914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "N03R-umjSi8A",
        "outputId": "c18f8ac7-5ff6-4cdf-c8f2-8fa6df5fa322"
      },
      "source": [
        "n = 2 # how many digits we will display\r\n",
        "plt.figure(figsize=(20, 4))\r\n",
        "for i in range(n):\r\n",
        "  ax=plt.subplot(2, n, i + 1)\r\n",
        "  plt.imshow(x_test[i].reshape(28, 28))\r\n",
        "  plt.gray()\r\n",
        "  ax.get_xaxis().set_visible(False)\r\n",
        "  ax.get_yaxis().set_visible(False)\r\n",
        "  ax=plt.subplot(2, n, i + 1 + n)\r\n",
        "  plt.imshow(decoded_imgs[i].reshape(28, 28))\r\n",
        "  plt.gray()\r\n",
        "  ax.get_xaxis().set_visible(False)\r\n",
        "  ax.get_yaxis().set_visible(False)\r\n",
        "plt.show()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHWD0svuSjL2"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error   ## we are finding error rate\r\n",
        "ms=mean_squared_error(x_test,decoded_imgs) \r\n",
        "MSE = np.square(np.subtract(x_test,decoded_imgs)).mean() \r\n",
        "MSE\r\n",
        "ms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW_y_8fewUmj",
        "outputId": "1fd2db58-7668-45bf-d2ea-a07487e4af49"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error ## we are finding error rate \r\n",
        "ms=mean_squared_error(x_test,decoded_imgs) \r\n",
        "MSE = np.square(np.subtract(x_test,decoded_imgs)).mean()\r\n",
        "MSE "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0095905885"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToRu5uByyN_i"
      },
      "source": [
        "Difference Between \"adam- optimizer\" and \"adadelta-optimizer\"\r\n",
        "**adaM**-It uses both first order moment mtmt and 2nd order moment gtgt but they are both decayed over time \r\n",
        "\r\n",
        "```\r\n",
        "\r\n",
        "```\r\n",
        "\r\n",
        "\r\n",
        "**adaDelta**-AdaDelta also uses exponentially decaying average of gtgt which was our 2nd moment of gradient."
      ]
    }
  ]
}